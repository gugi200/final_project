{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFxIglK2hYfo7busGDh5BP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugi200/final_project/blob/main/comp_diff_models_custom_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing libraries"
      ],
      "metadata": {
        "id": "yjfTlsw_j54-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFw-qaMkjwX1"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!wandb login 3014974e724f01c4d63f956fa13fd7f0463e16d4\n",
        "!pip install torchmetrics\n",
        "!pip install mlxtend>=0.19.0\n",
        "\n",
        "!pip list | grep mlx\n",
        "\n",
        "\n",
        "#\n",
        "#   Michael Gugala\n",
        "#   02/12/2023\n",
        "#   Image recognition\n",
        "#   Master 4th year project\n",
        "#   Univeristy of Bristol\n",
        "#\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets#\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchmetrics\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from sklearn.utils import Bunch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import requests\n",
        "import random\n",
        "import shutil\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from io import BytesIO, StringIO\n",
        "import os\n",
        "\n",
        "import wandb\n",
        "import cv2\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# check imports\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "#agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# downloading data and spittiling into raw train and test subsets"
      ],
      "metadata": {
        "id": "hMLHUUjfkAZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colabPath = Path(\"custom_dataset\")\n",
        "\n",
        "#  Create a dir\n",
        "if colabPath.is_dir():\n",
        "    print('directory already exists')\n",
        "else:\n",
        "    colabPath.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# download zipped data\n",
        "    github_path = \"https://github.com/gugi200/final_project/raw/main/data.zip\"\n",
        "    # file = \"dataset_pressure_sensor.zip\"\n",
        "    file = 'data.zip'\n",
        "with open(colabPath / file, \"wb\") as f:\n",
        "    request = requests.get(github_path)\n",
        "    f.write(request.content)\n",
        "\n",
        "\n",
        "# unzip the data\n",
        "with zipfile.ZipFile(colabPath / file, \"r\") as f:\n",
        "    f.extractall(colabPath)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TRAIN_RATIO = 0.75\n",
        "dirs = os.listdir(colabPath)\n",
        "\n",
        "\n",
        "#  Create a dir for train and test data\n",
        "extendedTrain = Path(\"train_raw\")\n",
        "extendedTest = Path(\"test_raw\")\n",
        "if extendedTrain.is_dir():\n",
        "    print('directory already exists')\n",
        "else:\n",
        "    extendedTrain.mkdir(parents=True, exist_ok=True)\n",
        "    extendedTest.mkdir(parents=True, exist_ok=True)\n",
        "    for dir in dirs:\n",
        "        path = extendedTrain / dir\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "    for dir in dirs:\n",
        "        path = extendedTest / dir\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for dir in dirs:\n",
        "    files = os.listdir(extendedDataPath / dir)\n",
        "    length = int(TRAIN_RATIO*len(files))\n",
        "    random.shuffle(files)\n",
        "\n",
        "    train_set = files[:length]\n",
        "    test_set = files[length:]\n",
        "\n",
        "    for data in train_set:\n",
        "        shutil.copy(extendedDataPath / dir / data, extendedTrain / dir / data)\n",
        "\n",
        "    for data in test_set:\n",
        "        shutil.copy(extendedDataPath / dir / data, extendedTest / dir / data)\n",
        "\n",
        "l = 0\n",
        "for dir in dirs:\n",
        "    l += len(os.listdir(extendedTrain/dir))\n",
        "    print(dir, len(os.listdir(extendedTrain/dir)))\n",
        "print(l)\n",
        "\n",
        "l = 0\n",
        "for dir in dirs:\n",
        "    l += len(os.listdir(extendedTest/dir))\n",
        "    print(dir, len(os.listdir(extendedTest/dir)))\n",
        "print(l)"
      ],
      "metadata": {
        "id": "kKGExjDVkE4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library"
      ],
      "metadata": {
        "id": "BL122EEfkEh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create data set from a custom data\n",
        "def create_dataset(path, batchsize, preprocess=None, mean=None, std=None):\n",
        "    '''\n",
        "    input:\n",
        "    path - path to the folder with the data\n",
        "           eg for train - \"data/FashionMNIST/train\"\n",
        "    batchsize - eg 32\n",
        "    mean (optional)- for normalization eg. [0.25, 0.25, 0.25]\n",
        "    std (optional)- for nortmalization eg [0.1, 0.1, 0.1]\n",
        "\n",
        "    returns:\n",
        "    dataloader with image size of 224\n",
        "    class_names\n",
        "    '''\n",
        "    if not preprocess:\n",
        "\n",
        "        if mean:\n",
        "            preprocess = transforms.Compose([\n",
        "\n",
        "                transforms.Resize(size=(224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=mean, std=std),\n",
        "            ])\n",
        "        else:\n",
        "            preprocess = transforms.Compose([\n",
        "\n",
        "                transforms.Resize(size=(224, 224)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "\n",
        "    data = datasets.ImageFolder(root=Path(path),\n",
        "                                    transform=preprocess, # tranform for the data\n",
        "                                    target_transform=None) # transform for label\n",
        "    dataloader = DataLoader(dataset=data,\n",
        "                                batch_size=batchsize,\n",
        "                                shuffle=True) #  shuffling to remove order\n",
        "    class_names = data.classes\n",
        "    return dataloader, class_names, data.targets\n",
        "\n",
        "# visdualize 9 random images in a batch\n",
        "def visualise_data(dataloader, class_names, batchsize):\n",
        "    '''\n",
        "    input dataloader\n",
        "    class names\n",
        "\n",
        "    displays 9 random images in a batch and their labels\n",
        "    '''\n",
        "    train_features_batch, train_labels_batch = next(iter(dataloader))\n",
        "    print(\"length of data: \", len(train_features_batch), 'length of labels: ', len(train_labels_batch))\n",
        "    # display random datapoints\n",
        "    fig = plt.figure(figsize=(9, 9))\n",
        "    rows, cols  = 3, 3\n",
        "    for pic in range(1, 1+rows*cols):\n",
        "        rand_int = np.random.randint(0, batchsize)\n",
        "        img = train_features_batch[rand_int]\n",
        "        img_RGB = img.permute([1, 2, 0]).numpy()\n",
        "        fig.add_subplot(rows, cols, pic)\n",
        "        plt.imshow(img_RGB.squeeze())\n",
        "        plt.axis(False)\n",
        "        plt.title(class_names[train_labels_batch[rand_int]])\n",
        "\n",
        "\n",
        "# create optimizer\n",
        "def create_optiimizer(model, optimizer, lr):\n",
        "    '''\n",
        "    inputs:\n",
        "    model - CNN network\n",
        "    optimizer - \"adam\" or \"sgd\"\n",
        "    lr - learning rate eg. 0.01\n",
        "\n",
        "    '''\n",
        "    if optimizer=='adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                     lr=lr\n",
        "                                     )\n",
        "    elif optimizer=='sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                     lr=lr,\n",
        "                                    momentum=0.9\n",
        "                                     )\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def get_lossFn():\n",
        "    '''\n",
        "        returns CrossEntropyLoss function\n",
        "    '''\n",
        "\n",
        "    return nn.CrossEntropyLoss()\n",
        "\n",
        "# create train step\n",
        "def train_step(model, metric, loss_fn, optimizer,\n",
        "               data_loader, device, debug=False, wnb=True):\n",
        "    '''\n",
        "    model - CNN network\n",
        "    metric - metric to calculate accuracy\n",
        "    loss_fn - loss function\n",
        "    optimizer - optimizer to be applied\n",
        "    data_loader - dataloader\n",
        "    device - decide for the model to train\n",
        "    debug (optional)- if True prints average loss and metric of the batch\n",
        "\n",
        "    returns\n",
        "    train_loss - average loss of the batch\n",
        "    train_acc - average metric score of the batch\n",
        "\n",
        "    The function saves the metric score and loss of each iteration in WandB\n",
        "\n",
        "    '''\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # put data on the device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        #forward pass, return raw logits\n",
        "        y_pred = model(X)\n",
        "\n",
        "        #loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        #accuracy\n",
        "        acc = metric(y, torch.argmax(y_pred, dim=1))\n",
        "\n",
        "        train_loss += loss # accumulate train loss\n",
        "        train_acc += acc # accumulate train accuracy\n",
        "\n",
        "        # zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # loss bacward\n",
        "        loss.backward()\n",
        "\n",
        "        #optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # log in wandb\n",
        "        if wnb:\n",
        "            wandb.log({\"loss\": loss,\n",
        "                       'accuracy': acc})\n",
        "\n",
        "    # device total loss and accuracy by length of train dataloader\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    if debug:\n",
        "        print(f'Train loss: {train_loss:.4f}, Train acc: {train_acc*100:0.4f}%')\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "# create test step\n",
        "def test_step(model, metric, loss_fn, data_loader, device, debug=False, wnb=True):\n",
        "    '''\n",
        "    model - CNN network\n",
        "    metric - metric to calculate accuracy\n",
        "    loss_fn - loss function\n",
        "    data_loader - dataloader\n",
        "    device - decide for the model to train\n",
        "    debug (optional)- if True prints average loss and metric of the batch\n",
        "\n",
        "    returns\n",
        "    test_loss - average loss of the batch\n",
        "    test_acc - average metric score of the batch\n",
        "\n",
        "    The function saves the metric score and loss of each iteration in WandB\n",
        "\n",
        "    '''\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in data_loader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            #1 forward pass\n",
        "            test_pred = model(X_test)\n",
        "\n",
        "            # calculate loss\n",
        "            loss = loss_fn(test_pred, y_test)\n",
        "            test_loss += loss\n",
        "\n",
        "            #accuracy\n",
        "            acc = metric(y_test, test_pred.argmax(dim=1))\n",
        "            test_acc += acc\n",
        "\n",
        "            if wnb:\n",
        "                wandb.log({\"test loss\": loss,\n",
        "                           'test accuracy': acc})\n",
        "\n",
        "        # Calculate the test loss average batch\n",
        "        test_loss /= len(data_loader)\n",
        "\n",
        "        # acc per bactch\n",
        "        test_acc /= len(data_loader)\n",
        "\n",
        "        # Print out what's happening\n",
        "        if debug:\n",
        "            print(f'Test loss: {test_loss:.4f}  |  Test acc: {test_acc*100:.4f}%')\n",
        "\n",
        "        return test_loss, test_acc\n",
        "\n",
        "# create evaluation loop\n",
        "def eval_model(model: torch.nn.Module,\n",
        "                data_loader: torch.utils.data.DataLoader,\n",
        "                loss_fn: torch.nn.Module,\n",
        "                accuracy_fn,\n",
        "               device):\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in tqdm(data_loader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred = model(X)\n",
        "\n",
        "            #accumulate the loss and acc\n",
        "            loss += loss_fn(y_pred, y)\n",
        "            acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
        "\n",
        "        # ave loss and acc\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "    return {\"model_name\": model.__class__.__name__, # only works if a model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc.item()*100}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_preds(model, dataloader, class_names, batchsize):\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    nrows = 3\n",
        "    ncols = 3\n",
        "    model = model.cpu()\n",
        "\n",
        "    dataL_len = len(dataloader)\n",
        "    data = iter(dataloader)\n",
        "    for i in range(3):\n",
        "        model.eval()\n",
        "        with torch.inference_mode():\n",
        "\n",
        "            X, y = next(data)\n",
        "            X, y = X.cpu(), y.cpu()\n",
        "            for j in range(3):\n",
        "                randint = np.random.randint(0, batchsize)\n",
        "                X_sample, y_sample = X[randint], y[randint]\n",
        "                pred_logit = model(X_sample.unsqueeze(dim=0))\n",
        "\n",
        "                pred_prob = pred_logit.argmax(dim=1)\n",
        "\n",
        "\n",
        "                plt.subplot(nrows, ncols, (3*i)+j+1);\n",
        "                plt.imshow(X_sample.squeeze().permute([1, 2, 0]), cmap='gray');\n",
        "\n",
        "                #find pred_label in text form\n",
        "                pred_label = class_names[pred_prob];\n",
        "\n",
        "                # find truth label\n",
        "                truth_label = class_names[y_sample];\n",
        "\n",
        "                title_text = f'Pred: {pred_label}  \\n  Truth: {truth_label}'\n",
        "\n",
        "                if pred_label==truth_label:\n",
        "                    plt.title(title_text, fontsize=10, c='g');\n",
        "                else:\n",
        "                    plt.title(title_text, fontsize=10, c='r');\n",
        "                plt.axis(False)\n",
        "                plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "def plot_decision_matrix(class_names, y_pred_tensor, targets):\n",
        "    # setup confusion matrix\n",
        "    confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "    confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                            target=targets)\n",
        "\n",
        "    # plot consufionmatrix\n",
        "    fig, ax = plot_confusion_matrix(\n",
        "        conf_mat=confmat_tensor.numpy(),\n",
        "        class_names=class_names,\n",
        "        figsize=(10, 7)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def make_predictions_dataloader(model, dataloader, device, class_names):\n",
        "    preds = []\n",
        "    target = []\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    metric = torchmetrics.classification.Accuracy(\n",
        "        task=\"multiclass\",\n",
        "        num_classes=len(class_names)\n",
        "    ).to(device)\n",
        "    with torch.inference_mode():\n",
        "\n",
        "        for X_test, y_test in tqdm(dataloader):\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "            batch_pred = model(X_test)\n",
        "            batch_pred = batch_pred.cpu()\n",
        "            y_test = y_test.cpu()\n",
        "            preds.append(np.array(batch_pred.argmax(dim=1)))\n",
        "            target.append(y_test)\n",
        "\n",
        "            acc = metric(y_test, batch_pred.argmax(dim=1))\n",
        "            test_acc += acc\n",
        "\n",
        "        # acc per bactch\n",
        "        test_acc /= len(dataloader)\n",
        "\n",
        "\n",
        "    return np.concatenate(preds), np.concatenate(target), test_acc\n",
        "\n",
        "\n",
        "def make_predictions(model, data, device):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "    model = model.to(device)\n",
        "    with torch.inference_mode():\n",
        "        y_preds = model(data)\n",
        "    return y_preds.cpu()\n",
        "\n",
        "\n",
        "def dataloader_to_numpy(dataloader):\n",
        "    for i, (data, target) in enumerate(dataloader):\n",
        "        if i==0:\n",
        "            data_numpy = data.numpy()\n",
        "            target_numpy = target.numpy()\n",
        "        else:\n",
        "            data_numpy = np.append(data_numpy, data.numpy(), axis=0)\n",
        "            target_numpy = np.append(target_numpy, target.numpy(), axis=0)\n",
        "    return data_numpy, target_numpy\n",
        "\n",
        "\n",
        "def get_datalodaer(batchsize, train_path, test_path):\n",
        "\n",
        "    train_dataloader, class_names, _ = create_dataset(\n",
        "                                        path=train_path,\n",
        "                                        batchsize=batchsize,\n",
        "                                    mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]\n",
        "                                    )\n",
        "    test_dataloader, _, _ = create_dataset(\n",
        "                                path=test_path,\n",
        "                                batchsize=batchsize,\n",
        "                                    mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]\n",
        "                                    )\n",
        "    return train_dataloader, test_dataloader, class_names\n",
        "\n",
        "\n",
        "def train_test_loop(config, model, train_dataloader, test_dataloader,\n",
        "                    class_names, device):\n",
        "    loss_fn = get_lossFn()\n",
        "    optimizer = create_optiimizer(model=model,\n",
        "                                    optimizer=config.optimizer,\n",
        "                                    lr=config.learning_rate\n",
        "    )\n",
        "    metric = torchmetrics.classification.Accuracy(\n",
        "        task=\"multiclass\",\n",
        "        num_classes=len(class_names)\n",
        "    ).to(device)\n",
        "    train_time_start = timer()\n",
        "    for epoch in range(config.epochs):\n",
        "        ave_batch_loss, ave_batch_metric = train_step(\n",
        "            model=model,\n",
        "            metric=metric,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            data_loader=train_dataloader,\n",
        "            device=device,\n",
        "            debug=True\n",
        "        )\n",
        "        ave_batch_loss, ave_batch_metric = test_step(\n",
        "            model=model,\n",
        "            metric=metric,\n",
        "            loss_fn=loss_fn,\n",
        "            data_loader=test_dataloader,\n",
        "            device=device,\n",
        "            debug=True\n",
        "        )\n",
        "        wandb.log({\"average train batch loss\": ave_batch_loss,\n",
        "                    \"average train batch metric\": ave_batch_metric,\n",
        "                    \"average test batch loss\": ave_batch_loss,\n",
        "                    \"average test batch metric\": ave_batch_metric,\n",
        "                    \"epoch\": epoch\n",
        "                    })\n",
        "    train_time_end = timer()\n",
        "    wandb.log({\"train time\": train_time_end - train_time_start})\n",
        "\n",
        "\n",
        "def train_test_loop_nonpipe(model, train_dataloader, test_dataloader, optim, lr,\n",
        "                    epochs, class_names, device):\n",
        "    loss_fn = get_lossFn()\n",
        "    optimizer = create_optiimizer(model=model,\n",
        "                                    optimizer=optim,\n",
        "                                    lr=lr\n",
        "    )\n",
        "    metric = torchmetrics.classification.Accuracy(\n",
        "        task=\"multiclass\",\n",
        "        num_classes=len(class_names)\n",
        "    ).to(device)\n",
        "    train_time_start = timer()\n",
        "    for epoch in range(epochs):\n",
        "        ave_batch_loss, ave_batch_metric = train_step(\n",
        "            model=model,\n",
        "            metric=metric,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            data_loader=train_dataloader,\n",
        "            device=device,\n",
        "            debug=True\n",
        "        )\n",
        "        ave_batch_loss, ave_batch_metric = test_step(\n",
        "            model=model,\n",
        "            metric=metric,\n",
        "            loss_fn=loss_fn,\n",
        "            data_loader=test_dataloader,\n",
        "            device=device,\n",
        "            debug=True\n",
        "        )\n",
        "        wandb.log({\"average train batch loss\": ave_batch_loss,\n",
        "                    \"average train batch metric\": ave_batch_metric,\n",
        "                    \"average test batch loss\": ave_batch_loss,\n",
        "                    \"average test batch metric\": ave_batch_metric,\n",
        "                    \"epoch\": epoch\n",
        "                    })\n",
        "    train_time_end = timer()\n",
        "    wandb.log({\"train time\": train_time_end - train_time_start})\n"
      ],
      "metadata": {
        "id": "zNOw4Ci0k6Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# models"
      ],
      "metadata": {
        "id": "hZoSZvB8k6-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## setup wandb"
      ],
      "metadata": {
        "id": "ohYFiINTldY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'loss',\n",
        "    'goal': 'minimize'\n",
        "    }\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'sgd']\n",
        "        },\n",
        "    'fc_layer_size': {\n",
        "        'values': [6]\n",
        "        },\n",
        "    }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "parameters_dict.update({\n",
        "    'epochs': {\n",
        "        'value': 5}\n",
        "    })\n",
        "\n",
        "# parameters_dict.update({\n",
        "#     'learning_rate': {\n",
        "#         # a flat distribution between 0 and 0.1\n",
        "#         'distribution': 'uniform',\n",
        "#         'min': 0,\n",
        "#         'max': 0.1\n",
        "#       },\n",
        "#     'batch_size': {\n",
        "#         # integers between 32 and 256\n",
        "#         # with evenly-distributed logarithms\n",
        "#         'distribution': 'q_log_uniform_values',\n",
        "#         'q': 8,\n",
        "#         'min': 8,\n",
        "#         'max': 32,\n",
        "#       }\n",
        "#     })\n",
        "\n",
        "parameters_dict.update({\n",
        "    'learning_rate': {\n",
        "        # a flat distribution between 0 and 0.1\n",
        "        'values': [0.001, 0.01, 0.1]\n",
        "      },\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms\n",
        "        'values': [16, 32]\n",
        "      }\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_O54qW5EldC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XWSbwaaflXLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model_resnet50(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        model = resnet50(\n",
        "            weights=ResNet50_Weights.DEFAULT).to(device)\n",
        "        model.fc = nn.Linear(2048 , config.fc_layer_size).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_alexnet(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('alexnet'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'alexnet', weight).to(device)\n",
        "        model.classifier[6] = nn.Linear(4096 , 10).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_convnext_base(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('convnext_base'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'convnext_base', weight).to(device)\n",
        "        model.classifier[2] = nn.Linear(1024 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_densenet161(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('densenet161'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'densenet161', weight).to(device)\n",
        "        model.classifier = nn.Linear(2208 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_efficientnet_v2_l(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('efficientnet_v2_l'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'efficientnet_v2_l', weight).to(device)\n",
        "        model.classifier[1] = nn.Linear(1280 , 10, bias=True).to(device).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_googlenet(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('googlenet'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'googlenet', weight).to(device)\n",
        "        model.fc = nn.Linear(1024 , 10, bias=True).to(device)\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_inception_v3(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('inception_v3'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'inception_v3', weight).to(device)\n",
        "        model.fc = nn.Linear(2048 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_maxvit_t(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('maxvit_t'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'maxvit_t', weight).to(device)\n",
        "        model.classifier[5] = nn.Linear(512 , 10, bias=False).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_mobilenet_v3_large(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('mobilenet_v3_large'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'mobilenet_v3_large', weight).to(device)\n",
        "        model.classifier[3] = nn.Linear(1280 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model_shufflenet_v2_x2_0(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('shufflenet_v2_x2_0'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'shufflenet_v2_x2_0', weight).to(device)\n",
        "        model.fc = nn.Linear(2048 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_swin_v2_t(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('swin_v2_t'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'swin_v2_t', weight).to(device)\n",
        "        model.head = nn.Linear(768 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_vgg19_bn(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('vgg19_bn'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'vgg19_bn', weight).to(device)\n",
        "        model.classifier[6] = nn.Linear(4096 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "def train_model_wide_resnet50_2(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_dataloader, test_dataloader, class_names = get_datalodaer(\n",
        "            train_data_path = shortTrain,\n",
        "            test_data_path = shortTest,\n",
        "            batchsize=config.batch_size)\n",
        "\n",
        "        weight = list(torchvision.models.get_model_weights('wide_resnet50_2'))[-1]\n",
        "        model = torch.hub.load('pytorch/vision', 'wide_resnet50_2', weight).to(device)\n",
        "        model.fc = nn.Linear(2048 , 10, bias=True).to(device)\n",
        "\n",
        "        train_test_loop(config=config,\n",
        "                        model=model,\n",
        "                        train_dataloader=train_dataloader,\n",
        "                        test_dataloader=test_dataloader,\n",
        "                        class_names=class_names\n",
        "                        )\n",
        "    return model\n",
        "\n",
        "\n",
        "count=10"
      ],
      "metadata": {
        "id": "5vRgWdyyk8Um"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}